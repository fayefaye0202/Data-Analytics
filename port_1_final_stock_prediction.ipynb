{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##port_1_julia_chan\n",
    "##stock price prediction system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'stock_prediction.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#show current working directory\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the working directory\n",
    "os.chdir('C:\\\\Users\\\\JC\\\\iCloudDrive\\\\Polyu\\\\COMP machine learning\\\\project\\\\project file\\\\20191224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training / Testing / Visualizing\n",
    "\"\"\"\n",
    "\n",
    "#libraries to use\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "import train\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#rom model_utils import ModelUtils\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify environment variable value of which info and warning messages logging output are ignored & not printed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters Setup\n",
    "\n",
    "# set training data file path \n",
    "price_train_dir = r'data\\raw_price_train'\n",
    "tweet_train_dir = r'data\\tweet_train'\n",
    "\n",
    "# set prediction data file path\n",
    "tweet_pred_dir = r'data\\tweet_test'\n",
    "\n",
    "# set training data date range start & end dates\n",
    "train_dates = ['2014-01-01', '2015-12-20']\n",
    "\n",
    "# set prediction data dates\n",
    "pred_dates = ['2015-12-21', '2015-12-22', '2015-12-23', '2015-12-24', '2015-12-28', '2015-12-29', '2015-12-30','2015-12-31']\n",
    "\n",
    "# set +ve integer number of previous days to predict price trend with time series\n",
    "window_size = 20\n",
    "\n",
    "# models to train\n",
    "models = [    \n",
    "    #'LinearRegression', \n",
    "    #'RidgeRegression', \n",
    "    #'LassoRegression',\n",
    "    #'SVRegression', \n",
    "    #'KNNRegression',  \n",
    "    #'AdaBoostLSTM',\n",
    "    'LongShortTermMemory']\n",
    "\n",
    "predict_model = 'LongShortTermMemory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training model \n",
    "\n",
    "class ModelUtils():\n",
    "\n",
    "    def __lstm_build__(x_shape_1, x_shape_2):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=200, activation='softsign', return_sequences=True, input_shape = (x_shape_1, x_shape_2)))\n",
    "        model.add(LSTM(units=200, activation='softsign', return_sequences=True, dropout=0.1))\n",
    "        model.add(LSTM(units=200, dropout=0.1))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "        return model\n",
    "\n",
    "    def isRegression(name):\n",
    "        return (name == 'LinearRegression' or name == 'RidgeRegression' or name == 'LassoRegression' or\n",
    "                name == 'SVRegression' or name == 'KNNRegression')\n",
    "\n",
    "    def isLSTM(name):\n",
    "        return (name == 'LongShortTermMemory' or name == 'AdaBoostLSTM')\n",
    "\n",
    "    def init(name, x_shape_1 = 0, x_shape_2 = 0):\n",
    "        if name == 'TfIdfVectorizer':\n",
    "            model = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.90, min_df=0.05)\n",
    "        elif name == 'LinearRegression':\n",
    "            model = LinearRegression()\n",
    "        elif name == 'RidgeRegression':\n",
    "            model = RidgeCV(alphas=np.logspace(-3, 3, 7), cv=5)\n",
    "        elif name == 'LassoRegression':\n",
    "            model = LassoCV(alphas=np.logspace(-3, 3, 7), cv=5)\n",
    "        elif name == 'SVRegression':\n",
    "            parameters = [{'kernel':['poly', 'rbf', 'sigmoid'], 'degree':[2, 3, 4], 'C':[1, 10, 100, 1000], 'epsilon':[0.05, 0.1]}]\n",
    "            model = GridSearchCV(SVR(gamma='scale'), parameters, cv=5)\n",
    "        elif name == 'KNNRegression':\n",
    "            model = GridSearchCV(KNeighborsRegressor(), {'n_neighbors': np.arange(1, 20)}, cv=5)\n",
    "        elif name == 'LongShortTermMemory':\n",
    "            model = KerasRegressor(build_fn=ModelUtils.__lstm_build__, x_shape_1=x_shape_1, x_shape_2=x_shape_2, epochs=120, batch_size=500)\n",
    "        elif name == 'AdaBoostLSTM':         \n",
    "            model = KerasRegressor(build_fn=ModelUtils.__lstm_build__, x_shape_1=x_shape_1, x_shape_2=x_shape_2, epochs=100, batch_size=500)\n",
    "            model = AdaBoostRegressor(base_estimator=model, n_estimators=20, learning_rate=0.2)\n",
    "        return model;\n",
    "\n",
    "    def print_result(name, model):\n",
    "        if name == 'LinearRegression':\n",
    "            pass\n",
    "        elif name == 'RidgeRegression':\n",
    "            print( \"RidgeRegression: alpha={}\".format(model.alpha_))\n",
    "        elif name == 'LassoRegression':\n",
    "            print( \"LassoRegression: alpha={}\".format(model.alpha_))\n",
    "        elif name == 'SVRegression':\n",
    "            print( \"SVRegression: kernel={}, degree={}, C={}, epsilon={}\".format(\n",
    "                model.best_params_['kernel'], model.best_params_['degree'], model.best_params_['C'], model.best_params_['epsilon']))\n",
    "        elif name == 'KNNRegression':\n",
    "            print( \"KNNRegresion: n={}\".format(model.best_params_['n_neighbors']))\n",
    "        elif name == 'LongShortTermMemory':\n",
    "            plt.plot(model.model.history.history['mse'])\n",
    "            plt.title('LSTM MSE vs Epoch')\n",
    "            plt.ylabel('mse')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.show()\n",
    "            pass\n",
    "        elif name == 'AdaBoostLSTM':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Open       High        Low      Close  Adj Close     Volume\n",
      "Date                                                                        \n",
      "2012-09-04  95.108574  96.448570  94.928574  96.424286  87.121140   91973000\n",
      "2012-09-05  96.510002  96.621429  95.657143  95.747147  86.509338   84093800\n",
      "2012-09-06  96.167145  96.898575  95.828575  96.610001  87.288956   97799100\n",
      "2012-09-07  96.864288  97.497147  96.538574  97.205711  87.827171   82416600\n",
      "2012-09-10  97.207146  97.612854  94.585716  94.677139  85.542564  121999500\n"
     ]
    }
   ],
   "source": [
    "## preview of price train data\n",
    "df = pd.read_csv('data/raw_price_train/1_r_price_train.csv', header=0, index_col=0)\n",
    "df.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.sort_index(ascending=True, axis=0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['rt', 'AT_USER', 'summary', 'of', \"yesterday's\", 'webcast', 'featuring', '$', 'aapl', '$', 'wynn', '$', 'goog', '$', 'lgf', 'tradereducation', 'options', 'hedgingstrategies', '-', '-', 'URL'], 'Wed Jan 01 03:59:03 +0000 2014'), (['rt', 'AT_USER', 'summary', 'of', \"yesterday's\", 'webcast', 'featuring', '$', 'aapl', '$', 'wynn', '$', 'goog', '$', 'lgf', 'tradereducation', 'options', 'hedgingstrategies', '-', '-', 'URL'], 'Wed Jan 01 03:29:29 +0000 2014'), (['itv', 'will', 'boost', 'apple', 'URL', '$', 'aapl', 'apple'], 'Wed Jan 01 18:08:47 +0000 2014'), (['iphone', 'users', 'are', 'more', 'intelligent', 'than', 'samsung', ',', 'blackberry', 'and', 'htc', 'owners', ',', '$', 'aapl', '$', 'bbry', ',', 'URL'], 'Wed Jan 01 01:52:31 +0000 2014'), (['rt', 'AT_USER', 'summary', 'of', \"yesterday's\", 'webcast', 'featuring', '$', 'aapl', '$', 'wynn', '$', 'goog', '$', 'lgf', 'tradereducation', 'options', 'hedgingstrategies', '-', '-', 'URL'], 'Wed Jan 01 01:18:36 +0000 2014'), (['2013', 'wrap-up', 'and', 'trading', 'set', 'review', '-', 'part', 'iii', 'URL', '$', 'aapl', 'apple', '$', 'bp', '$', 'cnw', '$', 'csco', '$', 'csx', '$', 'cvx', '$', 'goog', '$', 'hpq', '$', 'ibm', '$', 'intc', '$', 'ngg'], 'Wed Jan 01 10:52:20 +0000 2014'), (['apple', 'screwed', 'up', 'big', 'time', 'URL', '$', 'amzn', '$', 'aapl'], 'Wed Jan 01 15:01:12 +0000 2014'), (['rt', 'AT_USER', 'summary', 'of', \"yesterday's\", 'webcast', 'featuring', '$', 'aapl', '$', 'wynn', '$', 'goog', '$', 'lgf', 'tradereducation', 'options', 'hedgingstrategies', '-', '-', 'URL'], 'Tue Dec 31 23:10:08 +0000 2013')]\n"
     ]
    }
   ],
   "source": [
    "## preview of tweet train data\n",
    "content=[]\n",
    "with open('data/tweet_train/1_tweet_train/2014-01-01') as file:\n",
    "    for line in file.readlines():\n",
    "        content.append((json.loads(line)['text'],json.loads(line)['created_at']))\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training price & tweet data import\n",
    "\n",
    "def price_tweet_data_import():\n",
    "    \n",
    "    # get canonical price data file path and resolve symbolic links\n",
    "    price_train_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), price_train_dir)\n",
    "    price_train_file = glob.glob(os.path.join(price_train_path, \"*.csv\"))\n",
    "    \n",
    "    # get canonical tweet data file directory and resolve symbolic links\n",
    "    tweet_train_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), tweet_train_dir)\n",
    "    tweet_train_dir = glob.glob(os.path.join(tweet_train_path, \"*\"))\n",
    "    \n",
    "    # initialize train data dict\n",
    "    price_train_dict = {}\n",
    "    tweet_train_dict = {}\n",
    "    \n",
    "    # import price training data\n",
    "    for file in price_train_file:\n",
    "        df = pd.read_csv(file, header=0, index_col=0)\n",
    "        \n",
    "        # data cleaning\n",
    "        df.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)\n",
    "        \n",
    "        # set date index & sorting\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index(ascending=True, axis=0)\n",
    "        \n",
    "        #return stock ID of price data file\n",
    "        stock = os.path.basename(file)[:os.path.basename(file).find('_')]\n",
    "        \n",
    "        #extract stock price data dict within training date range\n",
    "        price_train_dict[stock] = df.loc[train_dates[0]:train_dates[1]]\n",
    "        \n",
    "    # import tweet training data\n",
    "    for dir in tweet_train_dir:\n",
    "        \n",
    "        # get tweet data file path\n",
    "        tweet_train_file = glob.glob(os.path.join(dir, \"*\"))\n",
    "        \n",
    "        # return stock ID of tweet data file\n",
    "        stock = os.path.basename(os.path.normpath(dir))[:os.path.basename(os.path.normpath(dir)).find('_')]\n",
    "        \n",
    "        # initialize list of tweet dates & content\n",
    "        all_date = []\n",
    "        all_content = []\n",
    "        \n",
    "        # extract tweet json file content by dates \n",
    "        for filename in tweet_train_file:\n",
    "            \n",
    "            #get tweet train dates\n",
    "            date = os.path.basename(filename)\n",
    "            \n",
    "            #read and save tweet content\n",
    "            content = []\n",
    "            with open(filename) as file:\n",
    "                for line in file.readlines():\n",
    "                    content.append((json.loads(line)['text'], json.loads(line)['created_at']))\n",
    "            \n",
    "            all_date.append(date)\n",
    "            all_content.append(content)\n",
    "        \n",
    "        # generate tweet training dataframe\n",
    "        df = pd.DataFrame(data=all_content, index=all_date, columns=['tweet'])\n",
    "        \n",
    "        # set date index & sorting\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index(ascending=True, axis=0)\n",
    "        \n",
    "        #extract tweet content dict within training date range\n",
    "        tweet_train_dict[stock] = df.loc[train_dates[0]:train_dates[1]]\n",
    "        \n",
    "    # Process tweets in non-trading days with no stock price\n",
    "    for stock, df in tweet_train_dict.items():\n",
    "        start_date = datetime.datetime.strptime(train_dates[0], '%Y-%m-%d')\n",
    "        end_date = datetime.datetime.strptime(train_dates[1], '%Y-%m-%d')\n",
    "        \n",
    "        prev_date = None\n",
    "        for date in (start_date + datetime.timedelta(n) for n in range(int((end_date - start_date).days + 1))):\n",
    "            if date in price_train_dict[stock].index:\n",
    "                prev_date = date\n",
    "            if date in tweet_train_dict[stock].index:\n",
    "                if not date in price_train_dict[stock].index:\n",
    "                    if not prev_date is None:\n",
    "                        if prev_date in tweet_train_dict[stock].index:\n",
    "                            df.at[prev_date, 'tweet'] = df.at[prev_date, 'tweet'] + df.at[date, 'tweet']\n",
    "                        else:\n",
    "                            df.at[prev_date, 'tweet'] = df.at[date, 'tweet']\n",
    "                    df.drop([date], inplace=True)\n",
    "        tweet_train_dict[stock] = df.sort_index(ascending=True, axis=0)\n",
    "        \n",
    "    return price_train_dict, tweet_train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tweet text data preprocessing\n",
    "\n",
    "def tweet_vectorizing_model_train(tweet_train_dict):\n",
    "    lines_dict = {\"all\": []}\n",
    "    \n",
    "    # Process Tweets using NLTK and RegEx\n",
    "    for stock, df in tweet_train_dict.items():\n",
    "        for index, row in df.iterrows():\n",
    "            words = []\n",
    "            for tweet, time in row['tweet']:\n",
    "                words.extend(tweet)\n",
    "            lines_dict[\"all\"].append(process_tweet_sentence(\" \".join(words)))\n",
    "            \n",
    "    # Train Vectorizor\n",
    "    for stock, lines in lines_dict.items():\n",
    "        vectorizer = ModelUtils.init('TfIdfVectorizer')\n",
    "        vectorizer.fit(lines) #vocabulary dictionary of all tokens in raw documents\n",
    "        print(\"Number of unique words extracted:{}\".format(len(vectorizer.vocabulary_))) #mapping of terms to feature indices\n",
    "        ModelUtils.save(vectorizer, \"{}-TfidfVectorizer.pkl\".format(stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature extraction and label preprocessing\n",
    "\n",
    "def features_label_preprocess(price_train_dict, tweet_train_dict):\n",
    "    X_dict = {}\n",
    "    y_dict = {}\n",
    "\n",
    "    for stock, df in price_train_dict.items():\n",
    "    \n",
    "        # Feature 1: Stock Closing Price\n",
    "        df_proc = df[['Adj Close']]\n",
    "\n",
    "        # Feature 2: Tweet Sentiment Score\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        tweet_scores = []\n",
    "\n",
    "        for date in df_proc.index:\n",
    "            scores = []\n",
    "            if (date in tweet_train_dict[stock].index):\n",
    "                for tweet, create_at in tweet_train_dict[stock].loc[date]['tweet']:\n",
    "                    score = analyser.polarity_scores(' '.join(tweet))\n",
    "                    if (score['compound'] < -0.05) or (score['compound'] > 0.05):\n",
    "                        scores.append(score['compound'])\n",
    "            if len(scores) > 0:\n",
    "                tweet_scores.append(sum(scores) / len(scores))\n",
    "            else:\n",
    "                tweet_scores.append(0)\n",
    "        df_proc['Tweet Score'] = tweet_scores\n",
    "\n",
    "        # Feature 3: Tweet Bag of Words TF-IDF\n",
    "        lines = []\n",
    "        for date in df_proc.index:\n",
    "            words = []\n",
    "            if (date in tweet_dict[stock].index):\n",
    "                for tweet, time in tweet_train_dict[stock].loc[date]['tweet']:\n",
    "                    words.extend(tweet)\n",
    "            lines.append(process_tweet_sentence(\" \".join(words)))\n",
    "        \n",
    "        vectorizer = ModelUtils.load('{}-TfidfVectorizer.pkl'.format(\"all\"))\n",
    "        df_vec = pd.DataFrame(data=vectorizer.transform(lines).toarray(), index=df_proc.index)\n",
    "        df_proc = pd.concat([df_proc, df_vec], axis='columns', join='inner')\n",
    "\n",
    "        # Feature 4: Weekday (Mon or Fri)\n",
    "        df_proc['Weekday'] = df.index.weekday\n",
    "        \n",
    "        # Feature 5: Stock Indicator\n",
    "        df_proc['Stock'] = stock\n",
    "        \n",
    "        # Label: Stock Price Daily Change (predict Next Day value)\n",
    "        df_proc['D+1 Change'] = df[['Adj Close']].pct_change().shift(-1)\n",
    "        \n",
    "        # Create preprocessed features & labels dict\n",
    "        df_proc.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "        np_proc = np.array(df_proc, dtype=np.float)\n",
    "\n",
    "        X, y = np_proc[:, :-1], np_proc[:,-1]\n",
    "\n",
    "        X_dict[stock] = X\n",
    "        y_dict[stock] = y\n",
    "\n",
    "    return X_dict, y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature transformation\n",
    "\n",
    "def features_transform(X_dict):\n",
    "    X_transform_dict = {}\n",
    "\n",
    "    for stock, x_array in X_dict.items():\n",
    "        transformers=[('normalize_close_price_feature', \n",
    "                        MinMaxScaler(), \n",
    "                        [0]),\n",
    "                      ('normalize_remaining_continuous_features', \n",
    "                        MinMaxScaler(), \n",
    "                        slice(1, x_array.shape[1] - 2)),\n",
    "                      (\"create_dummies_for_monday_and_friday\", \n",
    "                        OneHotEncoder(categories=[[0,4]], handle_unknown='ignore'), \n",
    "                        [x_array.shape[1] - 2]),\n",
    "                      (\"create_dummies_for_stock_indicators\", \n",
    "                        OneHotEncoder(categories=[[1,2,3,4,5,6,7,8]], drop='first'),  \n",
    "                        [x_array.shape[1] - 1])]\n",
    "\n",
    "        colTransformer = ColumnTransformer(transformers, remainder='passthrough')\n",
    "        np_proc = colTransformer.fit_transform(x_array)\n",
    "        X_transform_dict[stock] = np_proc\n",
    "\n",
    "    return X_transform_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature reshape\n",
    "\n",
    "def features_label_reshape(X_dict, y_dict, isLSTM=False):\n",
    "    X_reshape_dict = {}\n",
    "    y_reshape_dict = {}\n",
    "\n",
    "    for stock in X_dict.keys():\n",
    "\n",
    "        # Construct 3D feature matrix for LSTM model\n",
    "        if isLSTM:           \n",
    "            X_reshape = np.zeros((X_dict[stock].shape[0] - trend_days + 1, trend_days, X_dict[stock].shape[1]))\n",
    "        \n",
    "            for i in range(0, X_reshape.shape[0]):\n",
    "                for j in range(0, trend_days):\n",
    "                    for k in range(0, X_reshape.shape[2]):\n",
    "                        X_reshape[i, j, k] = X_dict[stock][i + trend_days - j - 1, k]\n",
    "\n",
    "            y_reshape = np.reshape(y_dict[stock][trend_days - 1:], (y_dict[stock].shape[0] - trend_days + 1, 1))\n",
    "\n",
    "        # Add past prices to 2D feature matrix for regression models\n",
    "        else:\n",
    "            X_reshape = np.zeros((X_dict[stock].shape[0] - trend_days + 1, X_dict[stock].shape[1] + trend_days - 1))\n",
    "        \n",
    "            for i in range(0, X_reshape.shape[0]):\n",
    "                for j in range(0, trend_days):\n",
    "                    X_reshape[i, j] = X_dict[stock][i + trend_days - j - 1, 0]\n",
    "\n",
    "                for j in range(trend_days, X_reshape.shape[1]):\n",
    "                    X_reshape[i, j] = X_dict[stock][i + trend_days - 1, j - trend_days + 1]\n",
    "\n",
    "            y_reshape = y_dict[stock][trend_days - 1:]\n",
    "\n",
    "        X_reshape_dict[stock] = X_reshape\n",
    "        y_reshape_dict[stock] = y_reshape\n",
    "\n",
    "    return X_reshape_dict, y_reshape_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test data split and merge \n",
    "\n",
    "def features_label_split_merge(X_dict, y_dict):\n",
    "    X_train_dict = {}\n",
    "    y_train_dict = {}\n",
    "    X_test_dict = {}\n",
    "    y_test_dict = {}\n",
    "\n",
    "    # Spliting for training/testing\n",
    "    for stock in X_dict.keys():\n",
    "\n",
    "        # Sorted Splitting\n",
    "        mid = round(X_dict[stock].shape[0] * 0.8)\n",
    "\n",
    "        X_train_dict[stock] = X_dict[stock][:mid]\n",
    "        X_test_dict[stock] = X_dict[stock][mid:]\n",
    "\n",
    "        y_train_dict[stock] = y_dict[stock][:mid]\n",
    "        y_test_dict[stock] = y_dict[stock][mid:]\n",
    "\n",
    "    # Merging training data\n",
    "    X_train = np.array([])\n",
    "    y_train = np.array([])\n",
    "\n",
    "    for stock in X_train_dict.keys():\n",
    "        if (X_train.size > 0):\n",
    "            X_train = np.r_[X_train, X_train_dict[stock]]\n",
    "            y_train = np.r_[y_train, y_train_dict[stock]]\n",
    "        else:\n",
    "            X_train = X_train_dict[stock]\n",
    "            y_train = y_train_dict[stock]\n",
    "\n",
    "    X_train_dict = { 'all': X_train }\n",
    "    y_train_dict = { 'all': y_train }\n",
    "    \n",
    "    return X_train_dict, X_test_dict, y_train_dict, y_test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model train and evaluation\n",
    "\n",
    "def model_train_evaluate(model_name, X_train_dict, X_test_dict, y_train_dict, y_test_dict, isLSTM=False):\n",
    "    \n",
    "    # Train Models\n",
    "    for stock in X_train_dict.keys():\n",
    "        model_dict = {}\n",
    "        if isLSTM:\n",
    "            model_dict[stock] = ModelUtils.init(model_name, X_train_dict[stock].shape[1], X_train_dict[stock].shape[2])\n",
    "            model_dict[stock].fit(X_train_dict[stock], y_train_dict[stock])\n",
    "        else:\n",
    "            model_dict[stock] = ModelUtils.init(model_name)\n",
    "            model_dict[stock].fit(X_train_dict[stock], y_train_dict[stock])\n",
    "\n",
    "    # Evaluate Models  \n",
    "    result_list = []\n",
    "    for stock in X_test_dict.keys():                                  \n",
    "        y_pred = model_dict['all'].predict(X_test_dict[stock])\n",
    "        if isLSTM:           \n",
    "            y_pred = np.reshape(y_pred, (y_pred.shape[0]))\n",
    "        y_test = np.reshape(y_test_dict[stock], (y_test_dict[stock].shape[0]))\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        result_list.append((y_test, y_pred, mse))\n",
    "    return result_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result_plot(result_list, axisLine=True):\n",
    "\n",
    "    col = len(result_list.keys())\n",
    "    if (col > 0):\n",
    "        row = len(list(result_list.values())[0])\n",
    "    \n",
    "        fig, axes = plt.subplots(nrows=row, ncols=col, figsize=(2+3*col, 1+row))\n",
    "        fig.canvas.set_window_title('Predicting data')\n",
    "\n",
    "        c = -1\n",
    "    \n",
    "        for (model_name, stock_list) in result_list.items():\n",
    "            c += 1\n",
    "            r = -1\n",
    "\n",
    "            for (y_true, y_pred, mse) in stock_list:\n",
    "                r += 1\n",
    "            \n",
    "                if col > 1:\n",
    "                    ax = axes[r, c]\n",
    "                elif row > 1:\n",
    "                    ax = axes[r]\n",
    "                else:\n",
    "                    ax = axes\n",
    "\n",
    "                ax.set_title(model_name + ' MSE={0:.5g}'.format(mse), fontsize=10)\n",
    "\n",
    "                ax.set_xticklabels(())\n",
    "                ax.set_yticklabels(())\n",
    "\n",
    "                ax.plot(y_true, 'b', label='actual')\n",
    "                ax.plot(y_pred, 'r', label='predict')\n",
    "        \n",
    "                if axisLine:\n",
    "                    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "                ax.fill_between(\n",
    "                    np.arange(0, len(y_true), 1),\n",
    "                    y_true,\n",
    "                    y_pred,\n",
    "                    color='r',\n",
    "                    alpha=0.2\n",
    "                )\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='lower right')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet_sentence(sentence): \n",
    "    stemmer = PorterStemmer()\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.extend([\"AT_USER\", 'URL', 'tradereducation', 'hedgingstrategies'])\n",
    "\n",
    "    sentence = re.sub(r'((((\\$ )+)(.*?) )|(\\$ (.*?))$|[^A-Za-z_\\s])', '', sentence)\n",
    "\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    words = [word for word in words if len(word) > 2 if not word in stopword_list]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    #words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *** Importing price/tweet data for training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b07485d5149a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  *** Importing price/tweet data for training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprice_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprice_tweet_data_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  *** Training tweet vectorizing models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-611e98b2591c>\u001b[0m in \u001b[0;36mprice_tweet_data_import\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# get canonical price data file path and resolve symbolic links\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprice_train_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprice_train_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprice_train_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprice_train_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    if len(sys.argv) > 1:\n",
    "        del sys.argv[0]\n",
    "        models = sys.argv\n",
    "\n",
    "    print('  *** Importing price/tweet data for training')\n",
    "    price_dict, tweet_dict = price_tweet_data_import()\n",
    "   \n",
    "    print('  *** Training tweet vectorizing models')\n",
    "    tweet_vectorizing_model_train(tweet_dict)\n",
    "\n",
    "    print('  *** Processing training data into features and label')\n",
    "    X_dict, y_dict = features_label_preprocess(price_dict, tweet_dict)\n",
    "\n",
    "    print('  *** Transforming training data features')\n",
    "    X_transform_dict = features_transform(X_dict)       \n",
    "\n",
    "    result_list = {};\n",
    "\n",
    "    data_ready = False\n",
    "    for name in models:\n",
    "        if ModelUtils.isRegression(name):\n",
    "            if not data_ready:\n",
    "                print('    - Regression Models')\n",
    "\n",
    "                print('  *** Reshaping features and label according to model requirement')\n",
    "                X_reshape_dict, y_reshape_dict = features_label_reshape(X_transform_dict, y_dict, False)\n",
    "\n",
    "                print('  *** Splitting features/labels for training/testing')\n",
    "                X_train_dict, X_test_dict, y_train_dict, y_test_dict = features_label_split_merge(X_reshape_dict, y_reshape_dict)\n",
    "\n",
    "                data_ready = True;\n",
    "\n",
    "            print('  *** Training and evaluating models')\n",
    "            result_list[name] = model_train_evaluate(name, X_train_dict, X_test_dict, y_train_dict, y_test_dict, False)\n",
    "\n",
    "    data_ready = False\n",
    "    for name in models:\n",
    "        if ModelUtils.isLSTM(name):\n",
    "            if not data_ready:\n",
    "                print('    - LSTM Models')\n",
    "\n",
    "                print('  *** Reshaping features and label according to model requirement')\n",
    "                X_reshape_dict, y_reshape_dict = features_label_reshape(X_transform_dict, y_dict, True)\n",
    "\n",
    "                print('  *** Splitting features/labels for training/testing')\n",
    "                X_train_dict, X_test_dict, y_train_dict, y_test_dict = features_label_split_merge(X_reshape_dict, y_reshape_dict)\n",
    "\n",
    "                data_ready = True;\n",
    "\n",
    "            print('  *** Training and evaluating models')\n",
    "            result_list[name] = model_train_evaluate(name, X_train_dict, X_test_dict, y_train_dict, y_test_dict, True)\n",
    "\n",
    "    print('  *** Visualizing model testing results')\n",
    "    model_result_plot(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_tweet_pred_data_import(price_dict, tweet_dict):\n",
    "    tweet_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), tweet_pred_dir)\n",
    "    tweet_pred_dirs = glob.glob(os.path.join(tweet_path, \"*\", \"\"))\n",
    "\n",
    "    # Create placeholder for price data\n",
    "    for price_df in price_dict.values():\n",
    "        for date in dates[:-1]:\n",
    "            price_df.loc[date] = [0, 0, 0, 0, 1, 0]\n",
    "        price_df.index = pd.to_datetime(price_df.index)\n",
    "\n",
    "    # Import tweet data       \n",
    "    for dir in tweet_pred_dirs:       \n",
    "        tweet_files = glob.glob(os.path.join(dir, \"*\"))\n",
    "        stock = os.path.basename(os.path.normpath(dir))[0]\n",
    "\n",
    "        for filename in tweet_files:\n",
    "            date = os.path.basename(filename)\n",
    "            content = []\n",
    "\n",
    "            with open(filename) as file:\n",
    "                for line in file.readlines():\n",
    "                    content.append((json.loads(line)['text'], json.loads(line)['created_at']))\n",
    "\n",
    "            tweet_dict[stock].at[date, 'tweet'] = content\n",
    "        tweet_dict[stock].index = pd.to_datetime(tweet_dict[stock].index)\n",
    "            \n",
    "    # Process tweets in non-trading days\n",
    "    for stock, tweet_df in tweet_dict.items():\n",
    "        start_date = datetime.datetime.strptime(dates[0], '%Y-%m-%d')\n",
    "        end_date = datetime.datetime.strptime(dates[-1], '%Y-%m-%d')\n",
    "\n",
    "        prev_date = start_date\n",
    "        while True:\n",
    "            prev_date = prev_date - datetime.timedelta(days=1)\n",
    "            if prev_date in price_dict[stock].index:\n",
    "                break\n",
    "\n",
    "        for date in (start_date + datetime.timedelta(n) for n in range(int ((end_date - start_date).days))):      \n",
    "            if date in price_dict[stock].index:\n",
    "                prev_date = date\n",
    "\n",
    "            if date in tweet_dict[stock].index:\n",
    "                if not date in price_dict[stock].index:\n",
    "\n",
    "                    if prev_date in tweet_dict[stock].index:\n",
    "                        tweet_df.at[prev_date, 'tweet'] = tweet_df.at[prev_date, 'tweet']  + tweet_df.at[date, 'tweet']\n",
    "                        \n",
    "                    else:\n",
    "                        tweet_df.at[prev_date, 'tweet'] = tweet_df.at[date, 'tweet']\n",
    "\n",
    "                    tweet_df.drop([date], inplace=True)\n",
    "                    tweet_dict[stock] = tweet_df.sort_index(ascending=True, axis=0)\n",
    "\n",
    "    return price_dict, tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_transform(X_dict):\n",
    "    X_transform_dict = {}\n",
    "    x_price_dict = {}\n",
    "\n",
    "    for stock, x_array in X_dict.items():\n",
    "        colTransformer = ModelUtils.load('{}-ColumnTransformer.pkl'.format(stock))\n",
    "\n",
    "        np_proc = x_array[x_array.shape[0] - (len(dates) + train.trend_days - 1):]\n",
    "        X_transform_dict[stock] = colTransformer.transform(np_proc)\n",
    "\n",
    "    return X_transform_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_predict(X_dict, isLSTM=False):\n",
    "    result_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), result_dir)\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    if isLSTM:\n",
    "        filename = \"{}-{}.joblib\".format(\"all\", predict_model)                         \n",
    "    else:\n",
    "        filename = \"{}-{}.pkl\".format(\"all\", predict_model)\n",
    "    model = ModelUtils.load(filename)\n",
    "\n",
    "    y_pred_dict = {}\n",
    "    for stock, X in X_dict.items():  \n",
    "        y_pred_dict[stock] = [];\n",
    "\n",
    "        transformer = ModelUtils.load('{}-ColumnTransformer.pkl'.format(stock)).transformers_[0][1]\n",
    "\n",
    "        if isLSTM:\n",
    "            price = transformer.inverse_transform([[X_dict[stock][0,0,0]]])[0,0]\n",
    "        else:\n",
    "            price = transformer.inverse_transform([[X_dict[stock][0,0]]])[0,0]\n",
    "\n",
    "        print(\"{} Day0 Close = {}\".format(stock, price))\n",
    "\n",
    "        for i in range(0, X.shape[0]):\n",
    "            if isLSTM:\n",
    "                y_pred = model.predict(X[i:(i+1),:,:])\n",
    "                x_price = X[i, 0, 0]\n",
    "            else:\n",
    "                y_pred = model.predict(X[i:(i+1),:])\n",
    "                x_price = X[i, 0]\n",
    "\n",
    "            y_pred = np.reshape(y_pred, (1, 1))\n",
    "            price = price * (1 + y_pred[0,0])\n",
    "\n",
    "            print(\"{} Next Close = {} | Y_pred = {}\".format(stock, price, y_pred))\n",
    "\n",
    "            y_transform = transformer.transform([[price]])\n",
    "\n",
    "            if i < (X.shape[0] - 1):\n",
    "                for j in range(i + 1, X.shape[0]):\n",
    "                    if isLSTM:\n",
    "                        X[j, j-i-1, 0] = y_transform[0,0]\n",
    "\n",
    "                    else:\n",
    "                        X[j, j-i-1] = y_transform[0,0]\n",
    "\n",
    "            y_pred_dict[stock].append(price)\n",
    "\n",
    "    result_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), result_dir)\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    y_pred_list = []\n",
    "    for list in y_pred_dict.values():\n",
    "        y_pred_list = y_pred_list + list\n",
    "\n",
    "    pickle.dump(y_pred_list, open(os.path.join(result_path, \"19005718G.pkl\"), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  \n",
    "    if len(sys.argv) > 1:\n",
    "        predict_model = sys.argv[1]\n",
    "\n",
    "    print('  *** Importing price/tweet data for predicting')\n",
    "    price_dict, tweet_dict = train.price_tweet_data_import()\n",
    "    price_tweet_pred_data_import(price_dict, tweet_dict)\n",
    "\n",
    "    print('  *** Processing data for prediction into features and label')\n",
    "    X_dict, y_dict = train.features_label_preprocess(price_dict, tweet_dict)\n",
    "\n",
    "    print('  *** Transforming training data features')\n",
    "    X_transform_dict = features_transform(X_dict)\n",
    "\n",
    "    print('  *** Reshaping features and label according to model requirement')\n",
    "    X_reshape_dict, y_reshape_dict = train.features_label_reshape(X_transform_dict, y_dict, ModelUtils.isLSTM(predict_model))\n",
    "\n",
    "    print('  *** Predicting stock price')\n",
    "    if ModelUtils.isLSTM(predict_model):\n",
    "        price_predict(X_reshape_dict, True)\n",
    "    elif ModelUtils.isRegression(predict_model):\n",
    "        price_predict(X_reshape_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit2c1687f2b73645c4a6a3d2d9ac2c947f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
