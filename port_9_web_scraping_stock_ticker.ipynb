{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping of stock tickers & company names\n",
    "#  fetch the html content of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of companies and stocks\n",
    "company_name = []\n",
    "company_ticker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.advfn.com/nyse/newyorkstockexchange.asp?companies=A'\n",
    "page = requests.get(URL)\n",
    "\n",
    "# parse the text response\n",
    "soup = BeautifulSoup(page.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preview of structure\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_rows = soup.find_all('tr', attrs={'class':'ts0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odd_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_rows = soup.find_all('tr', attrs={'class':'ts1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in odd_rows:\n",
    "    row = i.find_all('td')\n",
    "    company_name.append(row[0].text.strip()) #company name\n",
    "    company_ticker.append(row[1].text.strip()) #ticker\n",
    "    \n",
    "for i in even_rows:\n",
    "    row = i.find_all('td')\n",
    "    company_name.append(row[0].text.strip()) #company name\n",
    "    company_ticker.append(row[1].text.strip()) #ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to scrap the data\n",
    "def scrape_stock_symbols(Letter):\n",
    "    Letter = Letter.upper()\n",
    "    URL = 'https://www.advfn.com/nyse/newyorkstockexchange.asp?companies='+Letter\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.text,'html.parser')\n",
    "    odd_rows = soup.find_all('tr', attrs={'class':'ts0'})\n",
    "    even_rows = soup.find_all('tr', attrs={'class':'ts1'})\n",
    "    \n",
    "    for i in odd_rows:\n",
    "        row = i.find_all('td')\n",
    "        company_name.append(row[0].text.strip()) #company name\n",
    "        company_ticker.append(row[1].text.strip()) #ticker\n",
    "\n",
    "    for i in even_rows:\n",
    "        row = i.find_all('td')\n",
    "        company_name.append(row[0].text.strip()) #company name\n",
    "        company_ticker.append(row[1].text.strip()) #ticker\n",
    "\n",
    "    return (company_name,company_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# get company name and ticker start with 'b', reutrn\n",
    "\n",
    "(temp_name,temp_ticker) = scrape_stock_symbols('b')\n",
    "print(type((temp_name,temp_ticker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of every letter in alphabet\n",
    "import string\n",
    "string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every letter in alphabet to get all tickers and company names from the website\n",
    "for char in string.ascii_uppercase:\n",
    "    (temp_name, temp_ticker) = scrape_stock_symbols(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe that contain company ticker and name\n",
    "df = pd.DataFrame(columns = ['company_name','company_ticker'])\n",
    "df['company_name'] = temp_name\n",
    "df['company_ticker'] = temp_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A H Belo Corporation</td>\n",
       "      <td>AHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAC Holdings Inc</td>\n",
       "      <td>AAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABB Ltd</td>\n",
       "      <td>ABB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADC Therapeutics SA</td>\n",
       "      <td>ADCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEABridges Impact Corp</td>\n",
       "      <td>IMPX.U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>Zuora Inc</td>\n",
       "      <td>ZUO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>Zayo Group Holdings Inc</td>\n",
       "      <td>ZAYO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>Zimmer Biomet Holdings Inc</td>\n",
       "      <td>ZBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>Zoetis Inc</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>Zymeworks Inc</td>\n",
       "      <td>ZYME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3331 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company_name company_ticker\n",
       "0           A H Belo Corporation            AHC\n",
       "1               AAC Holdings Inc            AAC\n",
       "2                        ABB Ltd            ABB\n",
       "3            ADC Therapeutics SA           ADCT\n",
       "4         AEABridges Impact Corp         IMPX.U\n",
       "...                          ...            ...\n",
       "3326                   Zuora Inc            ZUO\n",
       "3327     Zayo Group Holdings Inc           ZAYO\n",
       "3328  Zimmer Biomet Holdings Inc            ZBH\n",
       "3329                  Zoetis Inc            ZTS\n",
       "3330               Zymeworks Inc           ZYME\n",
       "\n",
       "[3331 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the data, remove empty cells\n",
    "df = df[df['company_name']!= '']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43552\n"
     ]
    }
   ],
   "source": [
    "# part 2\n",
    "# fetch html content of webpage using requests and bs4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('https://www.usclimatedata.com/climate/united-states/us')\n",
    "print(len(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Climate United States - Normals and averages</title>\n",
      "Climate United States - Normals and averages\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text)\n",
    "print(soup.title)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"selection_title\">Select a state by name</p>\n",
      "Select a state by name\n",
      "<a class=\"navbar-brand\" href=\"/\" title=\"Temperature - Precipitation - Sunshine - Snowfall\"><img alt=\"Temperature - Precipitation - Sunshine - Snowfall\" data-src=\"https://www.usclimatedata.com/assets/images/us-climate-data.png\" height=\"34\" src=\"https://www.usclimatedata.com/assets/images/us-climate-data.png\" srcset=\"https://www.usclimatedata.com/assets/images/us-climate-data.png 1x, https://www.usclimatedata.com/assets/images/us-climate-data-2.png 2x\" width=\"31\"/><span class=\"white ml-2\">U.S. Climate Data</span></a>\n",
      "Temperature - Precipitation - Sunshine - Snowfall\n",
      "\n",
      "<div class=\"float-left mb-4 mt-2\"><p class=\"selection_title\">Select a state by name</p></div>\n"
     ]
    }
   ],
   "source": [
    "print(soup.p)\n",
    "print(soup.p.text)\n",
    "print(soup.a)\n",
    "print(soup.a['title'])\n",
    "print()\n",
    "print(soup.p.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify() is handy for formatted printing, but works only on bs4 objects, \n",
    "# not on strings, dicts or lists. For those you need to import pprint.\n",
    "\n",
    "# find_all anchor tags for, and print out the href attribute, which is the actual link url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "#\n",
      "/\n",
      "/climate/united-states/us\n",
      "/\n",
      "/climate/united-states/us\n",
      "/climate/alabama/united-states/3170\n",
      "/climate/alaska/united-states/3171\n",
      "/climate/arizona/united-states/3172\n",
      "/climate/arkansas/united-states/3173\n",
      "/climate/california/united-states/3174\n",
      "/climate/colorado/united-states/3175\n",
      "/climate/connecticut/united-states/3176\n",
      "/climate/delaware/united-states/3177\n",
      "/climate/district-of-columbia/united-states/3178\n",
      "/climate/florida/united-states/3179\n",
      "/climate/georgia/united-states/3180\n",
      "/climate/hawaii/united-states/3181\n",
      "/climate/idaho/united-states/3182\n",
      "/climate/illinois/united-states/3183\n",
      "/climate/indiana/united-states/3184\n",
      "/climate/iowa/united-states/3185\n",
      "/climate/kansas/united-states/3186\n",
      "/climate/kentucky/united-states/3187\n",
      "/climate/louisiana/united-states/3188\n",
      "/climate/maine/united-states/3189\n",
      "/climate/maryland/united-states/1872\n",
      "/climate/massachusetts/united-states/3191\n",
      "/climate/michigan/united-states/3192\n",
      "/climate/minnesota/united-states/3193\n",
      "/climate/mississippi/united-states/3194\n",
      "/climate/missouri/united-states/3195\n",
      "/climate/montana/united-states/919\n",
      "/climate/nebraska/united-states/3197\n",
      "/climate/nevada/united-states/3198\n",
      "/climate/new-hampshire/united-states/3199\n",
      "/climate/new-jersey/united-states/3200\n",
      "/climate/new-mexico/united-states/3201\n",
      "/climate/new-york/united-states/3202\n",
      "/climate/north-carolina/united-states/3203\n",
      "/climate/north-dakota/united-states/3204\n",
      "/climate/ohio/united-states/3205\n",
      "/climate/oklahoma/united-states/3206\n",
      "/climate/oregon/united-states/3207\n",
      "/climate/pennsylvania/united-states/3208\n",
      "/climate/puerto-rico/united-states/7335\n",
      "/climate/rhode-island/united-states/3209\n",
      "/climate/south-carolina/united-states/3210\n",
      "/climate/south-dakota/united-states/3211\n",
      "/climate/tennessee/united-states/3212\n",
      "/climate/texas/united-states/3213\n",
      "/climate/utah/united-states/3214\n",
      "/climate/vermont/united-states/3215\n",
      "/climate/virginia/united-states/3216\n",
      "/climate/washington/united-states/3217\n",
      "/climate/west-virginia/united-states/3218\n",
      "/climate/wisconsin/united-states/3219\n",
      "/climate/wyoming/united-states/3220\n",
      "/climate/washington/district-of-columbia/united-states/usdc0001\n",
      " https://facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
      "https://twitter.com/intent/tweet/?text=Climate United States - Normals and averages&url=https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
      "https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus&media=description=Climate United States - Normals and averages\n",
      "mailto:mail@example.com?subject=Climate United States - Normals and averages&body=Climate United States - Normals and averages - https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
      "https://www.tumblr.com/widgets/share/tool?posttype=link&title=Climate United States - Normals and averages&caption=&content=Climate United States - Normals and averages - https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus&canonicalUrl=https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus&shareSource=tumblr_share_button\n",
      "whatsapp://send?text='Climate United States - Normals and averages - https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus'\n",
      "https://www.facebook.com/yourweatherservice\n",
      "https://twitter.com/usclimatedata\n",
      "/website-info\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "## Filter urls using string functions\n",
    "# We just add an if to check conditions, then add the good ones to a list.\n",
    "#In the end we get 51 state links, including Washington DC.\n",
    "base_url = 'https://www.usclimatedata.com'\n",
    "state_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    url = link.get('href')\n",
    "    if url and '/climate/' in url and '/climate/united-states/us' not in url:\n",
    "        state_links.append(url)\n",
    "print(len(state_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Colorado - Temperature, Rainfall and Averages\n"
     ]
    }
   ],
   "source": [
    "#Test getting the data for one state, then print the title for that page.\n",
    "\n",
    "r = requests.get(base_url + state_links[5])\n",
    "soup = BeautifulSoup(r.text)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "rows = soup.find_all('tr')\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b97abb50a8be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mhigh_temps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhigh_temps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Filter rows, and add temp data to list\n",
    "# list comprehension to filter the rows.\n",
    "#Then we have only 2 rows left.\n",
    "#We iterate through those 2 rows, and add all the temps from data cells (td) into a list.\n",
    "\n",
    "\n",
    "rows = [row for row in rows if 'Average high' in str(row)]\n",
    "print(len(rows))\n",
    "\n",
    "high_temps = []\n",
    "for row in rows:\n",
    "    tds = row.find_all('td')\n",
    "    for i in range(1,7):\n",
    "        high_temps.append(tds[i].text)\n",
    "print(high_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorado\n",
      "Colorado\n"
     ]
    }
   ],
   "source": [
    "# Get the name of the State\n",
    "#First attempt we just split the title string into a list, and grab the second word.\n",
    "#But that doesn't work for 2-word states like New York and North Carolina.\n",
    "#So instead we slice the string from first blank to the hyphen.\n",
    "\n",
    "state = soup.title.string.split()[1]\n",
    "print(state)\n",
    "s = soup.title.string\n",
    "state = s[s.find(' '):s.find('-')].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Colorado': ['46', '54', '61', '72', '82']}\n"
     ]
    }
   ],
   "source": [
    "# Add state name and temp list to the data dictionary\n",
    "#For a single state, this is what our scraped data looks like.\n",
    "#In this example we only got monthly highs by stat\n",
    "#e, but you could drill into cities, and could get lows and precipitation.\n",
    "\n",
    "data = {}\n",
    "data[state] = high_temps\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b77e9355ca11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mhigh_temps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Put it all together and iterate 51 states\n",
    "#We loop through our 51-state list, and get high temp data for each state, and add it to the data dict.\n",
    "#This combines all our work above into a single for loop.\n",
    "#The result is a dict with 51 states and a list of monthly highs for each.\n",
    "\n",
    "data = {}\n",
    "for state_link in state_links:\n",
    "    url = base_url + state_link\n",
    "    r = requests.get(base_url + state_link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    rows = soup.find_all('tr')\n",
    "    rows = [row for row in rows if 'Average high' in str(row)]\n",
    "    high_temps = []\n",
    "    \n",
    "    for row in rows:\n",
    "        tds = row.find_all('td')\n",
    "        for i in range(1,7):\n",
    "            high_temps.append(tds[i].text)\n",
    "    s = soup.title.string\n",
    "    state = s[s.find(' '):s.find('-')].strip()\n",
    "    data[state] = high_temps\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file\n",
    "# write all this data to a CSV file\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('high_temps.csv','w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit2c1687f2b73645c4a6a3d2d9ac2c947f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
